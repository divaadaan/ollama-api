services:
  # Ollama service for local LLM
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 20s
      retries: 5
    restart: unless-stopped

  # Full build with telemetry
  llm-api-full:
    build:
      context: .
      target: full-build
    container_name: llm-api-full
    ports:
      - "8000:8000"
      - "8001:8001"  # Prometheus metrics
    environment:
      - LLM_API_URL=${LLM_API_URL:-http://ollama:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral}
      - TELEMETRY_ENABLED=true
      - MAX_TIMEOUT=${MAX_TIMEOUT:-60}
      - STARTUP_TIMEOUT=${STARTUP_TIMEOUT:-120}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
    volumes:
      - ./.env:/app/.env:ro
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped
    profiles:
      - full

  # Lightweight build without telemetry
  llm-api-lite:
    build:
      context: .
      target: lite-build
    container_name: llm-api-lite
    ports:
      - "8000:8000"
    environment:
      - LLM_API_URL=${LLM_API_URL:-http://ollama:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral}
      - TELEMETRY_ENABLED=false
      - MAX_TIMEOUT=${MAX_TIMEOUT:-60}
      - STARTUP_TIMEOUT=${STARTUP_TIMEOUT:-120}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
    volumes:
      - ./.env:/app/.env:ro
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout:  ${HEALTH_CHECK_TIMEOUT:-30}s
      retries: 3
      start_period: ${STARTUP_TIMEOUT:-120}s
    restart: unless-stopped
    profiles:
      - lite

  # Development build with hot reload
  llm-api-dev:
    build:
      context: .
      target: dev-build
    container_name: llm-api-dev
    ports:
      - "8000:8000"
      - "8001:8001"
    environment:
      - LLM_API_URL=http://ollama:11434/api/generate
      - DEFAULT_MODEL=mistral
      - TELEMETRY_ENABLED=true
      - TELEMETRY_CONSOLE_EXPORT=true
      - DEBUG=true
      - RELOAD=true
    volumes:
      - ./:/app  # Mount source for hot reload
      - ./.env:/app/.env:ro
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - dev

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    depends_on:
      - llm-api-full
    restart: unless-stopped
    profiles:
      - full
      - monitoring

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  ollama_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Usage examples:
# Full deployment with monitoring:
#   docker-compose --profile full --profile monitoring up -d
#
# Lightweight deployment:
#   docker-compose --profile lite up -d
#
# Development with hot reload:
#   docker-compose --profile dev up -d
#
# Just Ollama for external development:
#   docker-compose up ollama -d