services:
  ### Ollama service for local LLM uncomment if your machine is not running an ollama instance already###
  #ollama:
  #  image: ollama/ollama:latest
  #  container_name: ollama
  #  ports:
  #    - "11434:11434"
  #  volumes:
  #    - ollama_data:/root/.ollama
  #  environment:
  #    - OLLAMA_HOST=0.0.0.0
  #  healthcheck:
  #    test: ["CMD", "ollama", "list"]
  #    interval: 30s
  #    timeout: 20s
  #   retries: 5
  # restart: unless-stopped

  # Full build with telemetry
  llm-api-full:
    build:
      context: .
      target: full-build
    container_name: llm-api-full
    ports:
      - "8000:8000"
      - "8001:8001"  # Prometheus metrics
    environment:
      - LLM_API_URL=${LLM_API_URL:-http://ollama:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral}
      - TELEMETRY_ENABLED=true
      - TELEMETRY_METRICS_PORT=8001
      - TELEMETRY_CONSOLE_EXPORT=${TELEMETRY_CONSOLE_EXPORT:-false}
      - MAX_TIMEOUT=${MAX_TIMEOUT:-60}
      - STARTUP_TIMEOUT=${STARTUP_TIMEOUT:-120}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
    volumes:
      - ./.env:/app/.env:ro
    #depends_on:
    #  ollama:
    #    condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/simple"]
      interval: 30s
      timeout: 10s
      retries: 3
    #restart: unless-stopped
    profiles:
      - full

  # Lightweight build without telemetry
  llm-api-lite:
    build:
      context: .
      target: lite-build
    container_name: llm-api-lite
    ports:
      - "8000:8000"
    environment:
      - LLM_API_URL=${LLM_API_URL:-http://ollama:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral}
      - TELEMETRY_ENABLED=false
      - MAX_TIMEOUT=${MAX_TIMEOUT:-60}
      - STARTUP_TIMEOUT=${STARTUP_TIMEOUT:-120}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
    volumes:
      - ./.env:/app/.env:ro
    #depends_on:
    #  ollama:
    #    condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/simple"]
      interval: 30s
      timeout:  ${HEALTH_CHECK_TIMEOUT:-10}s
      retries: 3
      start_period: ${STARTUP_TIMEOUT:-120}s
    restart: unless-stopped
    profiles:
      - lite

  # Development build with hot reload
  llm-api-dev:
    build:
      context: .
      target: dev-build
    container_name: llm-api-dev
    ports:
      - "8000:8000"
      - "8001:8001"  # Prometheus metrics
    environment:
      # Core API Configuration (consistent with lite/full)
      - LLM_API_URL=${LLM_API_URL:-http://ollama:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral}
      - MAX_TIMEOUT=${MAX_TIMEOUT:-60}
      - STARTUP_TIMEOUT=${STARTUP_TIMEOUT:-120}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}

      # Telemetry Configuration
      - TELEMETRY_ENABLED=true
      - TELEMETRY_METRICS_PORT=8001
      - TELEMETRY_CONSOLE_EXPORT=true  # Enable for debugging

      # Development-Specific Settings
      - DEBUG=true
      - RELOAD=true
      - LOG_LEVEL=DEBUG
    volumes:
      - ./:/app  # Mount source for hot reload
      - ./.env:/app/.env:ro
    #depends_on:
    #  ollama:
    #    condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health/simple" ]
      interval: 30s
      timeout: ${HEALTH_CHECK_TIMEOUT:-10}s
      retries: 3
      start_period: ${STARTUP_TIMEOUT:-120}s
    restart: unless-stopped
    profiles:
      - dev

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    depends_on:
      - llm-api-full
    restart: unless-stopped
    profiles:
      - full
      - monitoring

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  #ollama_data:
  # driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Usage examples:
# Full deployment with monitoring:
#   docker-compose --profile full --profile monitoring up -d
#
# Lightweight deployment:
#   docker-compose --profile lite up -d
#
# Development with hot reload:
#   docker-compose --profile dev up -d
#
# Just Ollama for external development:
#   docker-compose up ollama -d